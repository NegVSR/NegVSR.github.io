<!-- <script src="https://www.google.com/jsapi" type="text/javascript"></script> -->
<!-- <script type="text/javascript">google.load("jquery", "1.3.2");</script> -->

<html>
<head>
    <title>NegVSR</title>
    <meta property="og:image" content="resources/teaser.jpeg"/>
    <meta property="og:title" content="Zero-shot Image-to-Image Translation"/>
    <meta property="og:description" content="Zero-shot Image-to-Image Translation"/>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.1/jquery.min.js"></script>
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.14.0/css/all.css">
    <link rel="stylesheet" href="styles/main.css">
    <!-- <link rel="stylesheet" href="styles/image_card_flip.css"> -->
    <link rel="stylesheet" href="styles/image_card_fader.css">
    <link rel="stylesheet" href="styles/image_card_slider.css">
</head>

<style>
    video{
        width: 100%;
        height: 100%;
    }
</style>

<body>
<br>

    <center>
    <span style="font-size:36px">NegVSR:Augmenting Negatives for Generalized Noise Modeling in Real-world Video Super-Resolution</span>
    <table align=center width=600px>
        
        <br>
        <table align=center width=250px>
            <tr>
                <td align=center width=120px>
                    <center>
                        <!-- <span style="font-size:24px"><a href='TODO'>[Paper]</a></span> -->
                        <span style="font-size:24px; margin-left: 0px;"><a href=''>[Code]</a></span>
                    </center>
                </td>
            </tr>
        </table>
    </table>
    </center>
    
    <hr>
    <center><h1>Abstract</h1></center>
    <p> 
	The capability of video super-resolution (VSR) to synthesize high-resolution (HR) video from ideal datasets has been demonstrated in many works. However, applying the VSR model to real-world video with unknown and complex degradation remains a challenging task. First, existing degradation metrics in most VSR methods are not able to effectively simulate real-world noise and blur. On the contrary, simple combinations of classical degradation are used for real-world noise modeling, which led to the VSR model often being violated by out-of-distribution noise. Second, many SR models focus on noise simulation and transfer. Nevertheless, the sampled noise is monotonous and limited. To address the aforementioned problems, we propose a Negatives augmentation strategy for generalized noise modeling in Video Super-Resolution (NegVSR) task. Specifically, we first propose sequential noise generation toward real-world data to extract practical noise sequences. Then, the degeneration domain is widely expanded by negative a	ugmentation to build up various yet challenging real-world noise sets. We further propose the augmented negative guidance loss to learn robust features among augmented negatives effectively. Extensive experiments on real-world datasets (e.g., VideoLQ and FLIR) show that our method outperforms state-of-the-art methods with clear margins, especially in visual quality. 
        <br>
        <span style="font-weight: 800;">Keywords:</span> real-world video super-resolution, noise sequences sampling, negative augmentation, augmented negative guidance
    
    </p>
    <hr>

    <br>
    <center><h1>Framework</h1></center>
    <center>
        <table align=center width=1000px>
            <tr>
                <td width=260px>
                    <center>
                        <img class="round" src="./assets/images/framework.jpg" style="width:900px"/>
                    </center>
                </td>
            </tr>
        </table>
    </center>
    <p> 
	The overview of the proposed NegMix. (a) Our approach initially extracts noise sequence N<sub>sq</sub> through window sequence C in an unsupervised manner. The motion of C occurs within the OOD video noise dataset V<sub>od</sub>. Subsequently, it mixes N<sub>sq</sub> and LR video V<sub>lr</sub> to create novel training input V<sub>lr</sub><sup>N</sup>. (b) V<sub>lr</sub><sup>N</sup> is applied with a patch-based random central rotation to derive V<sub>neg</sub>. (c) Both V<sub>neg</sub> and V<sub>lr</sub> are fed into the VSR model to generate <span style="font-family: 'Times New Roman', Times, serif; font-size: 18px; position: relative; top: 0px;">&#x0059;&#x0302;</span> and Y, respectively. And L<sub>Aug-P</sub> enables the model to recover more effective pixels from the V<sub>lr</sub>. L<sub>Aug-N</sub> drives Y to learn the robust features present in the negative output <span style="font-family: 'Times New Roman', Times, serif; font-size: 18px; position: relative; top: 0px;">&#x0059;&#x0302;</span>.
    </p> 
    


    <hr>
    <br>
    <h1>Method</h1>
    <center>
        <table align=center width=1000px style="margin-bottom: 20px;">
            <tr>
                <td width=1000px>
                    <center>
                        <img class="round" src="./assets/images/loss.jpg" style="width:1000px"/>
                    </center>
                </td>
            </tr>
        </table>
    </center>
    <p> 
	The figure depicts the process of our Augmented Negative Guidance approach. We obtain the positive output <span style="font-family: 'Times New Roman', Times, serif; font-size: 18px; position: relative; top: 0px;">&#x0059;&#x0302;</span> by passing V<sub>hr</sub> sequential through the degeneration model D and VSR. Then we inject noise sequence N<sub>sq</sub> into the degraded video and apply the video with negative augmentation. Finally, we encourage the model to learn robust features from the augmented noise and video by L<sub>Aug-N</sub> and L<sub>Aug-P</sub>.
    </p>
    <br>
    

    <center>
        <h2><span style="font-weight: bold;">Results: </span>compare with other method</h2>
        <table align=center width=1100px>
            <tr>
                <td width=260px>
                    <center>
                        <img class="round" src="./assets/images/comparison.jpg" style="width:1100px"/>
                    </center>
                </td>
            </tr>
        </table>

        <center><h2>More details:</h2></center>
        <table align=center width=1100px>
            <tr>
                <td width=260px>
                    <center>
                        <img class="round" src="./assets/images/appendix_comparsion.jpg" style="width:1100px"/>
                    </center>
                </td>
            </tr>
        </table>


        <hr>
        <center><h2>Video Demos:</h2></center>
        <table align=center width=1100px>
            <tr>
                <td width=260px>
                    <center>
                        <video controls>
                            <source src="assets/videos/023.mp4" type="video/mp4" style="width:1100px">
                        </video>
                    </center>
                </td>
            </tr>
        </table>
      
        <table align=center width=1100px>
            <tr>
                <td width=260px>
                    <center>
                        <video controls>
                            <source src="assets/videos/028.mp4" type="video/mp4" style="width:1100px">
                        </video>
                    </center>
                </td>
            </tr>
        </table>

    </center>
    
    
    


    <hr>

    <hr>
    <script>
        // $(".flip-card").click(function() {
        //     console.log("flipping")
        //   $(this).toggleClass("flipped");
        // });

        // $(".flip-card").mouseenter(function() {
        //     console.log("flipping")
        //   $(this).toggleClass("flipped");
        // });

        // $(".flip-card").mouseleave(function() {
        //     console.log("flipping")
        //   $(this).toggleClass("flipped");
        // });

        $(".flip-card").mouseenter(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("out");
            $(div_front).removeClass("in");

            $(div_back).addClass("in");
            $(div_back).removeClass("out");

        });

        $(".flip-card").mouseleave(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("in");
            $(div_front).removeClass("out");

            $(div_back).addClass("out");
            $(div_back).removeClass("in");

        });



        // $(".card-front").click(function() {
        //     card_back = $(this).next()[0];
        //     $(card_back).animate({
        //         width: "toggle",
        //     }, "slow");
            
        // });
    </script>
    <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    
</body>
</html>

